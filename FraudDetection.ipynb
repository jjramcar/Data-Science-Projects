{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FraudDetection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHLs07g8ah3n0lOcXPUONN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjramcar/Data-Science-Projects/blob/main/FraudDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud Detection\n",
        "Here we will train several models and evaluate how effectively they predict instances of fraud using data based on this dataset from Kaggle.\n",
        "\n",
        "Each row in fraud_data.csv corresponds to a credit card transaction. Features include confidential variables V1 through V28 as well as Amount which is the amount of the transaction.\n",
        "\n",
        "The target is stored in the class column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
      ],
      "metadata": {
        "id": "6agq3aZkj6vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oehV6UXhkRFa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuxwC9bij5WL",
        "outputId": "b36be968-89b1-46fd-bbc6-1c57e660b036"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.016410823768035772"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "def task_one():\n",
        "    df = pd.read_csv(r'/content/fraud_data.csv')\n",
        "    \n",
        "    total = df['Class'].count()\n",
        "    fr=df['Class'].value_counts()\n",
        "       \n",
        "    return fr[1]/total\n",
        "\n",
        "task_one()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(r'/content/fraud_data.csv')\n",
        "\n",
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "6ZEz_yLxlQqa",
        "outputId": "4539d554-ebd3-4ecb-a50e-6f3b018ac3cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             V1        V2        V3        V4        V5        V6        V7  \\\n",
              "7681  -1.332095  0.828224  1.280394 -0.000192  0.006864  0.193420  0.065110   \n",
              "21329  1.900589 -0.850905 -0.856684  0.419809 -0.329338  0.512227 -0.644830   \n",
              "14695  2.242056 -1.496697 -1.006464 -1.448708 -1.440162 -1.035270 -1.011986   \n",
              "12338 -0.308109  1.107050 -0.184132 -0.528948  1.000317 -0.517253  0.935125   \n",
              "17768 -1.405410  1.985541 -1.290536 -1.672087  1.133642 -1.327069  1.839333   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "13123  2.108057 -0.068298 -1.637994 -0.075459  0.785052  0.170575  0.063027   \n",
              "19648 -1.139866  1.130677  0.573881  1.034586  0.177193 -0.011452 -0.018946   \n",
              "9845   1.795059 -0.456973 -2.049603  0.241425  0.347659 -0.217058  0.000488   \n",
              "10799  1.185560 -0.062333  0.173661 -0.532931 -0.681227 -0.949888 -0.267220   \n",
              "2732  -0.772959 -0.238565  0.633944 -1.115754  1.400879  2.886209  1.118523   \n",
              "\n",
              "             V8        V9       V10  ...       V20       V21       V22  \\\n",
              "7681  -0.694045  0.020374  0.275932  ... -0.361099  0.632575 -0.286266   \n",
              "21329  0.083636 -0.622460  1.100660  ... -0.393216 -0.234496 -0.263067   \n",
              "14695 -0.251838 -1.060550  1.598898  ... -0.443243 -0.170011 -0.090092   \n",
              "12338  0.012976 -0.150110 -1.081891  ...  0.055325  0.044064  0.465492   \n",
              "17768 -0.605819  1.243372  2.321808  ...  0.881069 -0.058432  0.731687   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "13123 -0.075121  0.202474  0.209343  ... -0.090014 -0.304134 -0.765574   \n",
              "19648  0.679649 -0.231098 -0.634133  ... -0.016407 -0.218900 -0.610938   \n",
              "9845   0.053710  0.971166 -0.695139  ...  0.006272 -0.172207 -0.612954   \n",
              "10799 -0.034664  1.528210 -1.577029  ... -0.134965 -0.018312  0.231974   \n",
              "2732   0.494974 -0.043695 -0.884815  ...  0.265795 -0.021244  0.010725   \n",
              "\n",
              "            V23       V24       V25       V26       V27       V28  Amount  \n",
              "7681   0.283056  0.019710 -0.570700  0.409895 -0.669576 -0.374803    4.00  \n",
              "21329  0.151624  0.170524 -0.142015 -0.703641  0.049886 -0.021393   96.00  \n",
              "14695  0.194057 -0.155795 -0.199022 -0.153737 -0.005144 -0.049103   50.00  \n",
              "12338 -0.370183 -0.561096 -0.152336  0.590986  0.376048  0.248322    3.79  \n",
              "17768 -0.273287 -0.427056 -0.032592  0.073411  0.779993  0.247911    1.54  \n",
              "...         ...       ...       ...       ...       ...       ...     ...  \n",
              "13123  0.220031 -0.326253 -0.134034  0.208094 -0.074702 -0.072162    0.89  \n",
              "19648 -0.233344 -0.490935 -0.117678 -0.453945  0.206407  0.069866   19.90  \n",
              "9845   0.077735  0.073546 -0.115195 -0.116337 -0.042548 -0.012938  121.36  \n",
              "10799 -0.053601  0.319664  0.556433 -0.615846  0.102273  0.053048    7.00  \n",
              "2732   0.335805 -2.747197 -0.623597  0.185425 -0.052732 -0.084813  200.00  \n",
              "\n",
              "[16269 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1f71954-9793-4f5a-bc46-188f7c068253\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>...</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7681</th>\n",
              "      <td>-1.332095</td>\n",
              "      <td>0.828224</td>\n",
              "      <td>1.280394</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>0.193420</td>\n",
              "      <td>0.065110</td>\n",
              "      <td>-0.694045</td>\n",
              "      <td>0.020374</td>\n",
              "      <td>0.275932</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.361099</td>\n",
              "      <td>0.632575</td>\n",
              "      <td>-0.286266</td>\n",
              "      <td>0.283056</td>\n",
              "      <td>0.019710</td>\n",
              "      <td>-0.570700</td>\n",
              "      <td>0.409895</td>\n",
              "      <td>-0.669576</td>\n",
              "      <td>-0.374803</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21329</th>\n",
              "      <td>1.900589</td>\n",
              "      <td>-0.850905</td>\n",
              "      <td>-0.856684</td>\n",
              "      <td>0.419809</td>\n",
              "      <td>-0.329338</td>\n",
              "      <td>0.512227</td>\n",
              "      <td>-0.644830</td>\n",
              "      <td>0.083636</td>\n",
              "      <td>-0.622460</td>\n",
              "      <td>1.100660</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.393216</td>\n",
              "      <td>-0.234496</td>\n",
              "      <td>-0.263067</td>\n",
              "      <td>0.151624</td>\n",
              "      <td>0.170524</td>\n",
              "      <td>-0.142015</td>\n",
              "      <td>-0.703641</td>\n",
              "      <td>0.049886</td>\n",
              "      <td>-0.021393</td>\n",
              "      <td>96.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14695</th>\n",
              "      <td>2.242056</td>\n",
              "      <td>-1.496697</td>\n",
              "      <td>-1.006464</td>\n",
              "      <td>-1.448708</td>\n",
              "      <td>-1.440162</td>\n",
              "      <td>-1.035270</td>\n",
              "      <td>-1.011986</td>\n",
              "      <td>-0.251838</td>\n",
              "      <td>-1.060550</td>\n",
              "      <td>1.598898</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.443243</td>\n",
              "      <td>-0.170011</td>\n",
              "      <td>-0.090092</td>\n",
              "      <td>0.194057</td>\n",
              "      <td>-0.155795</td>\n",
              "      <td>-0.199022</td>\n",
              "      <td>-0.153737</td>\n",
              "      <td>-0.005144</td>\n",
              "      <td>-0.049103</td>\n",
              "      <td>50.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12338</th>\n",
              "      <td>-0.308109</td>\n",
              "      <td>1.107050</td>\n",
              "      <td>-0.184132</td>\n",
              "      <td>-0.528948</td>\n",
              "      <td>1.000317</td>\n",
              "      <td>-0.517253</td>\n",
              "      <td>0.935125</td>\n",
              "      <td>0.012976</td>\n",
              "      <td>-0.150110</td>\n",
              "      <td>-1.081891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.055325</td>\n",
              "      <td>0.044064</td>\n",
              "      <td>0.465492</td>\n",
              "      <td>-0.370183</td>\n",
              "      <td>-0.561096</td>\n",
              "      <td>-0.152336</td>\n",
              "      <td>0.590986</td>\n",
              "      <td>0.376048</td>\n",
              "      <td>0.248322</td>\n",
              "      <td>3.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17768</th>\n",
              "      <td>-1.405410</td>\n",
              "      <td>1.985541</td>\n",
              "      <td>-1.290536</td>\n",
              "      <td>-1.672087</td>\n",
              "      <td>1.133642</td>\n",
              "      <td>-1.327069</td>\n",
              "      <td>1.839333</td>\n",
              "      <td>-0.605819</td>\n",
              "      <td>1.243372</td>\n",
              "      <td>2.321808</td>\n",
              "      <td>...</td>\n",
              "      <td>0.881069</td>\n",
              "      <td>-0.058432</td>\n",
              "      <td>0.731687</td>\n",
              "      <td>-0.273287</td>\n",
              "      <td>-0.427056</td>\n",
              "      <td>-0.032592</td>\n",
              "      <td>0.073411</td>\n",
              "      <td>0.779993</td>\n",
              "      <td>0.247911</td>\n",
              "      <td>1.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13123</th>\n",
              "      <td>2.108057</td>\n",
              "      <td>-0.068298</td>\n",
              "      <td>-1.637994</td>\n",
              "      <td>-0.075459</td>\n",
              "      <td>0.785052</td>\n",
              "      <td>0.170575</td>\n",
              "      <td>0.063027</td>\n",
              "      <td>-0.075121</td>\n",
              "      <td>0.202474</td>\n",
              "      <td>0.209343</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090014</td>\n",
              "      <td>-0.304134</td>\n",
              "      <td>-0.765574</td>\n",
              "      <td>0.220031</td>\n",
              "      <td>-0.326253</td>\n",
              "      <td>-0.134034</td>\n",
              "      <td>0.208094</td>\n",
              "      <td>-0.074702</td>\n",
              "      <td>-0.072162</td>\n",
              "      <td>0.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19648</th>\n",
              "      <td>-1.139866</td>\n",
              "      <td>1.130677</td>\n",
              "      <td>0.573881</td>\n",
              "      <td>1.034586</td>\n",
              "      <td>0.177193</td>\n",
              "      <td>-0.011452</td>\n",
              "      <td>-0.018946</td>\n",
              "      <td>0.679649</td>\n",
              "      <td>-0.231098</td>\n",
              "      <td>-0.634133</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.016407</td>\n",
              "      <td>-0.218900</td>\n",
              "      <td>-0.610938</td>\n",
              "      <td>-0.233344</td>\n",
              "      <td>-0.490935</td>\n",
              "      <td>-0.117678</td>\n",
              "      <td>-0.453945</td>\n",
              "      <td>0.206407</td>\n",
              "      <td>0.069866</td>\n",
              "      <td>19.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9845</th>\n",
              "      <td>1.795059</td>\n",
              "      <td>-0.456973</td>\n",
              "      <td>-2.049603</td>\n",
              "      <td>0.241425</td>\n",
              "      <td>0.347659</td>\n",
              "      <td>-0.217058</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.053710</td>\n",
              "      <td>0.971166</td>\n",
              "      <td>-0.695139</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006272</td>\n",
              "      <td>-0.172207</td>\n",
              "      <td>-0.612954</td>\n",
              "      <td>0.077735</td>\n",
              "      <td>0.073546</td>\n",
              "      <td>-0.115195</td>\n",
              "      <td>-0.116337</td>\n",
              "      <td>-0.042548</td>\n",
              "      <td>-0.012938</td>\n",
              "      <td>121.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10799</th>\n",
              "      <td>1.185560</td>\n",
              "      <td>-0.062333</td>\n",
              "      <td>0.173661</td>\n",
              "      <td>-0.532931</td>\n",
              "      <td>-0.681227</td>\n",
              "      <td>-0.949888</td>\n",
              "      <td>-0.267220</td>\n",
              "      <td>-0.034664</td>\n",
              "      <td>1.528210</td>\n",
              "      <td>-1.577029</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134965</td>\n",
              "      <td>-0.018312</td>\n",
              "      <td>0.231974</td>\n",
              "      <td>-0.053601</td>\n",
              "      <td>0.319664</td>\n",
              "      <td>0.556433</td>\n",
              "      <td>-0.615846</td>\n",
              "      <td>0.102273</td>\n",
              "      <td>0.053048</td>\n",
              "      <td>7.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2732</th>\n",
              "      <td>-0.772959</td>\n",
              "      <td>-0.238565</td>\n",
              "      <td>0.633944</td>\n",
              "      <td>-1.115754</td>\n",
              "      <td>1.400879</td>\n",
              "      <td>2.886209</td>\n",
              "      <td>1.118523</td>\n",
              "      <td>0.494974</td>\n",
              "      <td>-0.043695</td>\n",
              "      <td>-0.884815</td>\n",
              "      <td>...</td>\n",
              "      <td>0.265795</td>\n",
              "      <td>-0.021244</td>\n",
              "      <td>0.010725</td>\n",
              "      <td>0.335805</td>\n",
              "      <td>-2.747197</td>\n",
              "      <td>-0.623597</td>\n",
              "      <td>0.185425</td>\n",
              "      <td>-0.052732</td>\n",
              "      <td>-0.084813</td>\n",
              "      <td>200.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16269 rows Ã— 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1f71954-9793-4f5a-bc46-188f7c068253')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1f71954-9793-4f5a-bc46-188f7c068253 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1f71954-9793-4f5a-bc46-188f7c068253');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def task_two():\n",
        "    from sklearn.dummy import DummyClassifier\n",
        "    from sklearn.metrics import recall_score\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    \n",
        "    dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
        "    \n",
        "    y_dummy_predictions = dummy_majority.predict(X_test)\n",
        "    \n",
        "    return (accuracy_score(y_test, y_dummy_predictions),recall_score(y_test, y_dummy_predictions))\n",
        "\n",
        "task_two()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StuUE-Zqle-w",
        "outputId": "51d2f9e5-9032-4811-81c9-681486409e81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9852507374631269, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 3\n",
        "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
        "\n",
        "This function should a return a tuple with three floats, i.e. (accuracy score, recall score, precision score)."
      ],
      "metadata": {
        "id": "aVhrlc7blq0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task_three():\n",
        "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    svm = SVC().fit(X_train, y_train)\n",
        "    svm_predicted = svm.predict(X_test) \n",
        "       \n",
        "    return (accuracy_score(y_test, svm_predicted), recall_score(y_test, svm_predicted), precision_score(y_test,svm_predicted))\n",
        "\n",
        "task_three()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3KOx-exlvnI",
        "outputId": "7f0e5c7e-892f-48a6-fbc1-8106e0debcab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9900442477876106, 0.35, 0.9333333333333333)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 4\n",
        "Using the SVC classifier with parameters {'C': 1e9, 'gamma': 1e-07}, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
        "\n",
        "This function should return a confusion matrix, a 2x2 numpy array with 4 integers."
      ],
      "metadata": {
        "id": "2JZ8YCMLmCmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def task_four():\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    svm = SVC(C=1e9, cache_size=200, class_weight=None, coef0=0.0,\n",
        "              decision_function_shape='ovo', degree=3, gamma=1e-07, kernel='rbf',\n",
        "              max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "              tol=0.001, verbose=False).fit(X_test, y_test)\n",
        "    y_scores_svm = svm.decision_function(X_test)\n",
        "    \n",
        "    y_scores_svm = np.where(y_scores_svm > -220, 1, 0)\n",
        "    confusion= confusion_matrix(y_test, y_scores_svm)\n",
        "    \n",
        "    return print(confusion)\n",
        "\n",
        "task_four()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFfojyGhmKeP",
        "outputId": "14b36f2e-b0d9-4190-a796-2915fb7ba856"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5170  174]\n",
            " [   7   73]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 5\n",
        "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
        "\n",
        "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
        "\n",
        "Looking at the precision recall curve, what is the recall when the precision is 0.75?\n",
        "\n",
        "Looking at the roc curve, what is the true positive rate when the false positive rate is 0.16?\n",
        "\n",
        "This function should return a tuple with two floats, i.e. (recall, true positive rate)."
      ],
      "metadata": {
        "id": "Zb9W6OWLnYXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "    \n",
        "lr = LogisticRegression().fit(X_train, y_train) \n",
        "y_scores_lr = lr.predict(X_test)\n",
        "y_proba_lr = lr.predict_proba(X_test) \n",
        "y_proba_lr = y_proba_lr[0:5424,1]\n",
        "    #y_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1]))\n",
        "    \n",
        "    \n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_lr)\n",
        "closest_zero = np.argmin(np.abs(thresholds))\n",
        "closest_zero_p = precision[closest_zero]\n",
        "closest_zero_r = recall[closest_zero]\n",
        "plt.figure()\n",
        "plt.xlim([0.73, 0.8])\n",
        "plt.ylim([0.82, 0.85])\n",
        "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
        "#plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
        "plt.xlabel('Precision', fontsize=16)\n",
        "plt.ylabel('Recall', fontsize=16)\n",
        "plt.axes().set_aspect('equal')\n",
        "plt.show() \n",
        "    \n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
        "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlim([0.0, 0.175])\n",
        "plt.ylim([0.85, 0.95])\n",
        "plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
        "plt.xlabel('False Positive Rate', fontsize=16)\n",
        "plt.ylabel('True Positive Rate', fontsize=16)\n",
        "    #plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
        "    #plt.legend(loc='lower right', fontsize=13)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
        "plt.axes().set_aspect('equal')\n",
        "#plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "Y5tGBTBvngnt",
        "outputId": "fb06966e-6f6e-4092-cdd2-c91ade594cb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAADxCAYAAAA3MOvfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwcVbn/8c83gbCERSCoQMABBTQii8a4XREBEcHL5sIOUa/LBVQQlc0LCCKoeAUFvYKyJSoqKEaJIgFDlB9qwhYJkBiGAElQEwIKQQnL8/vjnJGi0zOpyVR3V2a+79erX1N1Ti3P9CxP1zlV5ygiMDMzK2NYpwMwM7NVh5OGmZmV5qRhZmalOWmYmVlpThpmZlbaap0OoJVGjRoVXV1dnQ7DzGyVcuutty6OiI2b1Q3qpNHV1cWMGTM6HYaZ2SpF0gO91bl5yszMSnPSMDOz0pw0zMysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKw0Jw0zMyvNScPMzEpz0jAzs9KcNMzMrDQnDTMzK81Jw8zMSnPSMDOz0tqeNCTtKWm2pLmSTmxS/zJJN0iaKWmqpNEN9etJmi/pgvZFbWZm0OakIWk4cCHwLmAMcLCkMQ2bnQtcERHbA2cAZzfUnwlMa3WsZma2vHZfaYwD5kZEd0QsA64E9m3YZgxwY17+TbFe0uuAlwC/bkOsZmbWoN1JYzPgocL6/FxWdCdwQF7eH1hX0kaShgFfBT7d8ijNzKypOnaEfxp4m6TbgbcBC4BngaOAyRExv6+dJX1E0gxJMxYtWtT6aM3MhpB2zxG+ANi8sD46l/1bRCwkX2lIWgd4T0Q8JulNwFslHQWsA4yQ9EREnNiw/0XARQBjx46Nln0nZmZDULuTxnRga0lbkpLFQcAhxQ0kjQKWRMRzwEnAJQARcWhhm/HA2MaEYWZmrdXW5qmIeAY4BrgOuAf4UUTMknSGpH3yZrsAsyXNIXV6n9XOGM3MrHeKGLwtOGPHjo0ZM2Z0Ogwzs1WKpFsjYmyzujp2hJuZWU31O2lIWic/tb16KwIyM7P6Kp00JL1b0m3A34H7gNfk8u9IOqTPnc3MbFAolTQk7Qf8DFgMnNCw3/3AkdWHZmZmdVP2SuM04NKI2AM4r6HuLmC7SqMyM7NaKps0XgX8MC833m71KLBRZRGZmVltlU0a/wBG9VLXBXi8DjOzIaBs0rgeOEnSiwplIWkN0sN6v6w8MjMzq52yw4icAvwRmA1MJjVRnQhsD6wP7NeS6MzMrFZKXWlExDzgtcAvgHeQRp3dGfg98IY8yKCZmQ1ypQcszEOSf6iFsZiZWc2VfU7jRkmv7KVuG0k3NqszM7PBpWxH+C7Aer3UrUuaLMnMzAa5/ow91dtwuC8HnqggFjMzq7le+zQkfQD4QF4N4CJJjzdsthbpafAbWhOemZnVSV9XGs+R7pJ6FlDDes/rEeBbuIPczGxI6PVKIyIuBy4HkPQb4L8j4t52BWZmZvVT6pbbiHh7qwMxM7P6K/2cBoCkHYBtgTUb6yLiiqqCMjOzeiqVNPKYU9cCb+wpyl+Ld1Q5aZiZDXJlb7n9Imn4851JCWN/YFfge0A3MK4l0ZmZWa2UTRrvJCWO3+f1+RExNSKOAKYAn2xFcGZmVi9l+zQ2Aboj4llJ/yI9Bd7jJ8CVlUdmZrYKu3haN+dNmcPSZc92OpRKlb3S+AvQM5fGA8CbCnWvqDQiM7NBYDAmDCh/pfE7Uif4L4AJwGmSuoBngCOBSa0IzsxsVTUYEwaUTxqfBzbNy18hdYofCKxNShgfrz40M7PBYd45e3c6hH7Rl3qvK/tw333AfXn5aeD4/DIzsyGkP6PcNiVpJ0k/rSIYMzOrtz6vNCQNB14HbAHcFxG3F+rGAqcBewGNo9+amdkg1OuVhqTRwB+AW4AfATMk/VDSCEnfyXW7Al8lzalhZmaDXF/NU+cArwT+h3Q1cQzwZuBm4IOkYUO2iojPRsQjZU8oaU9JsyXNlXRik/qXSbpB0kxJU3PyQtKOkm6RNCvXHVj+2zQzsyr01Ty1G3B6RJzbUyBpNukJ8G9ERL+fAs/NXRcC7wDmA9MlTYqIuwubnQtcERGXS9oVOBs4HHgSOCIi/ixpU+BWSddFxGP9jcPMzFZOX1caG/P8sCE9bslff7yS5xsHzI2I7ohYRnqSfN+GbcYAN+bl3/TUR8SciPhzXl4I/C3HaGZmbdJX0hgGLGso61l/ciXPtxnwUGF9fi4ruhM4IC/vD6wraaPiBpLGASPItwE31H1E0gxJMxYtWrSSYZqZWTMrek7jPyVtV1gfRhoOfR9JOxY3jIhLKorp08AFksYD04AFpKllAZC0Cemp9CMj4rnGnSPiIuAigLFjx0ZjvZmZrbwVJY1Teik/tWE9gDJJYwGweWF9dC57/kCp6ekAAEnrAO/p6beQtB5pXo9TIqKx6czMzFqsr6SxZQvONx3YWtKWpGRxEHBIcQNJo4Al+SriJHIykjQC+Cmpk/yqFsRmZmYr0GvSiIgHqj5ZRDwj6RjgOmA4cElEzJJ0BjAjIiYBuwBnSwpS89TReff3kyaB2ig3XQGMj4g7qo7TzMya69cc4VWIiMnA5IayUwvLVwHLXUlExERgYssDNDOzXg147CkzMxs6nDTMzKw0Jw0zMyvNScPMzErrV9KQNEzSdpLeJmlkq4IyM7N6Kp00JB0N/IU0zMeNwLa5/BpJn2hNeGZmVielkoakDwPnA9eQ5gZXofq3wHuqD83MzOqm7HManwK+GhEn5OHNi+4FPlNtWGZWFxdP6+a8KXNYuuzZFW9sg17Z5qktSU9xN7MUeFE14ZhZ3ThhDMzIEY2fs1dtZZPGYqCrl7ptaRh00MwGDyeMlTdyxHCO3X2bTodRqbLNU78ATpU0FegZkyry4ILHkfo6zGyQm3fO3p0OwTqs7JXG54CngLtI070G8HXgHtJcF2e0JDozM6uVUkkjIhYDY0nzda9OmjFvNeAC4E0R8feWRWhmZrVRepTbiHgcODO/zMxsCCr7nMbXGqd3NTOzoadsn8Z44FZJd0n6rKTNWhiTmZnVVNmk8RLSzHlzSc1TD0iaIukIj0FlZjZ0lO0IXxYRV0fEfsAmwCeAtYDLgL9KmtC6EM3MrC76PTR6RCyJiG9GxFuAtwOPAodUHpmZmdVOv+cIz81R7wUOA3YBngGurjYsMzOro7J3Tw2TtKek7wF/BS4B1gD+G3hpRLy/hTGamVlNlL3SWAhsTOoIPweYGBHzWhWUmZnVU9mkcRUwISL+0MpgzMys3koljYg4ptWBmJlZ/fWaNCTtDNwWEU/k5T5FxLRKIzMzs9rp60pjKvBG4I95OXrZTrlucM00YmZmy+krabwduDsv70rvScPMzIaIXpNGRNxUWJ7almjMzKzWyj6n0S1ph17qtpPUXW1YZmZWR2WHEekiPczXzJrAy8qeMD8kOFvSXEknNql/maQbJM2UNFXS6ELdkZL+nF9Hlj2nmZlVoz9jT/XWpzEWeKzMASQNBy4E3gWMAQ6WNKZhs3OBKyJie9I0smfnfTcETgPeAIwDTpO0QT/iNzOzAerrltvjgOPyagA/l7SsYbO1gA2BK0uebxwwNyK68zmuBPbl+Q53SMnkU3n5N8A1efmdwPURsSTvez2wJ/CDkuc2swYXT+vmvClzWLrs2U6HYquIvu6e6gZuyMtHAjOARQ3bPEX6h/+dkufbDHiosD6fdOVQdCdwAHA+sD+wrqSNetl3ucmgJH0E+AjAFltsUTIss6GpPwlj5AjfVW993z31M+BnAJIAzoiI+9sQ06eBCySNB6YBC4DSH4Mi4iLgIoCxY8f6NmGzPvQnYRy7+zYtjsZWBWWHEflARedbAGxeWB+dy4rnWki60kDSOsB7IuIxSQtIQ7EX951aUVxmQ968c/budAi2CuirT+NU4DsRsTAv9yUi4swS55sObC1pS1KyOIiGCZwkjQKWRMRzwEmkYdgBrgO+WOj83iPXm5lZm/R1pXE68CvSsOinr+A4QZo7vO+NIp6RdAwpAQwHLomIWZLOAGZExCTS1cTZkoLUPHV03neJpDNJiQdSc9mSFZ3TzMyq01efxrBmywMVEZOByQ1lpxaWryINxd5s30t4/srDzMzarLJkYGZmg1/ZYUS2kTSusL6WpLMl/Tw3N5mZ2RBQ9krjAuC9hfWzgOOBTYGvSTq66sDMzKx+yiaNHYCbASQNA44AToiI1wFfID9MZ2Zmg1vZpLE+8Ehe3gnYgOc7q6cCW1UblpmZ1VHZpPFX4BV5eQ/gvojoGdJjHeCZqgMzM7P6KfVEODCJ9OzEdsB44NuFuteQxqkyM7NBrmzSOJE0b8Y7SQnki4W6fYBfVxyXmZnVUNmxp5YCH+6l7s2VRmRmZrVV9koD+PdESG8izaGxBLjFQ3mYmQ0dpZOGpC+Qns0oTvv6lKRzI+J/Ko/MzMxqp+wT4ccCJwMTgbcDr8pfJwInS/pEyyI0M7PaKHul8THg/Ig4rlA2G7hJ0hPAUcDXqw7OzMzqpexzGl3Atb3UXZvrzcxskCubNB4Btuul7tU8/7S4mZkNYmWTxk+BMyUdLmk1AEmrSToYOAO4ulUBmplZfZRNGicBdwCXA/+U9Ffgn8D3gDtJneRmZjbIlX2473FJOwN7A2/l+ec0bgJ+GRHRuhDNBp+Lp3Vz3pQ5LF32bKdDMeuXPpOGpFHAYaTBCh8Fro6IE9oRmNlgVreEMXLE8E6HYKuIXpOGpG2BacDGheITJb03In7W8sjMBrG6JYxjd9+m02HYKqKvK40vAP8CdgGmk+bMuAj4X8BJw6wi887Zu9MhmJXWV0f4G4BTI2JaRPwzImYBHwW6JG3cx35mZjZI9ZU0NiM99V00GxBpbnAzMxti+koaAhobXp8rsZ+ZmQ1SK7rl9vOSFhfWlb+eKak4JHpExJHVhmZmZnXTV9J4kDSabaMHSEOHFPk5DTOzIaDXpBERXW2Mw8zMVgHumzAzs9KcNMzMrLS2Jw1Je0qaLWmupBOb1G8h6TeSbpc0U9JeuXx1SZdL+pOkeySd1O7YzcyGurYmDUnDgQuBdwFjgIMljWnY7HPAjyJiJ+Ag4Ju5/H3AGhHxGuB1wEcldbUjbjMzS9p9pTEOmBsR3RGxDLgS2LdhmwDWy8vrAwsL5SPzfB5rAcuAf7Q+ZDMz69HupLEZ8FBhfX4uKzodOEzSfGAy8PFcfhWwFHiYdDvwuRGxpGFfJH1E0gxJMxYtWlRx+GZmQ1u/koak7SUdI+k0SS/NZa+QtG6FMR0MXBYRo4G9gAmShpGuUp4lDWGyJXC8pK0ad46IiyJibESM3XhjD5FlZlalUpMwSVoDmAgcQHoqPICfA38BvgzMAZbr1G5iAbB5YX10Liv6ELAnQETcImlNYBRwCPCriHga+Jukm4GxQHeZ78HMzAau7JXGWcDuwOHAS3h+OBGAXwLvLHmc6cDWkraUNILU0T2pYZsHgd0AJL0KWBNYlMt3zeUjgTcC95Y8r5mZVaBs0jgY+FxEfJ80zWvR/UBXmYNExDPAMcB1wD2ku6RmSTpD0j55s+OBD0u6E/gBMD5PJ3shsI6kWaTkc2lEzCwZv5mZVaBU8xSwEemffDPDgDXKnjAiJpM6uItlpxaW7wbe0mS/J0i33ZqZWYeUvdK4H3hTL3XjWH7eDTMzG4TKJo0rSPODHwqsnstC0tuB44BLWhGcmZnVS9mk8WXgWmAC8Ggu+x0whXRH0zdaEJuZmdVMqT6NiHgWOEjShaQ7pV4MPEJKGDe1MD6zjrp4WjfnTZnD0mWNk1iaDU1lO8IBiIjfAr9tUSxmtdPqhDFyxPCWHdusFTw0ulkfWp0wjt19m5Yd36wVyj4R/hwrmNI1IvyRyQa1eefs3ekQzDqubPPUGSyfNDYC9iA9o3FZhTGZmVlNle0IP71ZeZ4f4+fA3yuMyczMampAfRr5rqpvAsdWE46ZmdVZFR3hawAbVnAcMzOrubId4Vs0KR4BbAecA8yoMigzM6unsh3h82h+95SA+4CjqwrIzMzqq2zS+ECTsn8BDwDTc9+GmZkNcitMGvkOqTuAhRHhSbfNzIawMh3hQeqz2KnFsZiZWc2tMGlExHPAQ8DI1odjZmZ1VvaW228Dx+Z5vc3MbIgq2xG+LvByoFvSr4CHeeHdVBERp1UdnJmZ1UuvSUNSN7B/RNwJnFyo+mCTzQNw0jAzG+T6utLoIj3tTUR4CHUzM/N8GmZmVt6Kkkafc2iYmdnQsqKO8M9LWlziOBERR1YRkJmZ1deKksaOwFMljuMrEjOzIWBFSWO/iPhjWyIxM7Pac0e4mZmVVvbhPrOOuXhaN+dNmcPSZR5M2azTfKVhtVeHhDFyxPCOnt+sLnpNGhExrBX9GZL2lDRb0lxJJzap30LSbyTdLmmmpL0KddtLukXSLEl/krRm1fFZ/dQhYRy7+zYdjcGsLtraPJXn5rgQeAcwH5guaVJE3F3Y7HPAjyLiW5LGAJOBLkmrAROBwyPiTkkbAU+3M37rvHnn7N3pEMyGtHY3T40D5kZEd0QsA64E9m3YJoD18vL6wMK8vAcwM4+FRUQ84hkDzczaq91JYzPS3Bw95ueyotOBwyTNJ11lfDyXbwOEpOsk3Sbps60O1szMXqiOHeEHA5dFxGhgL2CCpGGkprT/AA7NX/eXtFvjzpI+ImmGpBmLFnl2WjOzKrU7aSwANi+sj85lRR8CfgQQEbcAawKjSFcl0yJicUQ8SboKeW3jCSLioogYGxFjN9544xZ8C2ZmQ1e7k8Z0YGtJW+ZZAA8CJjVs8yCwG4CkV5GSxiLgOuA1ktbOneJvA+7GzMzapq13T0XEM5KOISWA4cAlETFL0hnAjIiYBBwPXCzpOFKn+PiICOBRSf9LSjwBTI6Ia9sZv5nZUNf2J8IjYjKpaalYdmph+W7gLb3sO5F0262ZmXVAHTvCzcysppw0zMysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKw0Jw0zMyvNScPMzEpz0jAzs9KcNMzMrDQnDTMzK81Jw8zMSnPSMDOz0to+yq3V38XTujlvyhyWLvMU7Gb2QkpTVQxOkh4HZnc6jl6MAhZ3OogmHFf/1TU2x9U/dY0L2h/byyKi6dSng/1KY3ZEjO10EM1ImlHH2BxX/9U1NsfVP3WNC+oVm/s0zMysNCcNMzMrbbAnjYs6HUAf6hqb4+q/usbmuPqnrnFBjWIb1B3hZmZWrcF+pWFmZhVy0jAzs9JW2aQhaU9JsyXNlXRik/o1JP0w1/9BUleh7qRcPlvSO+sQl6R3SLpV0p/y112rjGsgsRXqt5D0hKRP1yUuSdtLukXSrPzerdnpuCStLunyHM89kk6qKqaSce0s6TZJz0h6b0PdkZL+nF9HVhnXQGKTtGPh5zhT0oF1iKtQv56k+ZIuqEtc+e/x1/l37O7Gv9eWiYhV7gUMB+4DtgJGAHcCYxq2OQr4v7x8EPDDvDwmb78GsGU+zvAaxLUTsGle3g5YUJf3rFB/FfBj4NN1iIv0nNFMYIe8vlFNfpaHAFfm5bWBeUBXG+PqArYHrgDeWyjfEOjOXzfIyxu0+WfZW2zbAFvn5U2Bh4EXdTquQv35wPeBC+rwfuW6qcA78vI6wNpVxdbXa1W90hgHzI2I7ohYBlwJ7Nuwzb7A5Xn5KmA3ScrlV0bEUxFxPzA3H6+jcUXE7RGxMJfPAtaStEZFcQ0oNgBJ+wH359iqNJC49gBmRsSdABHxSERUNfbJQOIKYKSk1YC1gGXAP9oVV0TMi4iZwHMN+74TuD4ilkTEo8D1wJ4VxTWg2CJiTkT8OS8vBP4GNH0iuZ1xAUh6HfAS4NcVxTPguCSNAVaLiOvzdk9ExJMVx9fUqpo0NgMeKqzPz2VNt4mIZ4C/kz6Jltm3E3EVvQe4LSKeqiiuAcUmaR3gBODzFcYz4LhIn05D0nX5Ev6zNYnrKmAp6dPyg8C5EbGkjXG1Yt+2HV/SONIn7/s6HZekYcBXgUqbZAcaF+l3/zFJP5F0u6SvSBpeeYRNDPZhRFY5kl4NfIn0KbouTge+FhFP5AuPulgN+A/g9cCTwA2Sbo2IGzobFuOAZ0nNLBsAv5U0JSK6OxtW/UnaBJgAHBkRy33q74CjgMkRMb+Gv/tvJTVrPwj8EBgPfLfVJ15VrzQWAJsX1kfnsqbb5GaC9YFHSu7bibiQNBr4KXBERFT1KauK2N4AfFnSPOBY4GRJx9QgrvnAtIhYnC/NJwOvrUFchwC/ioinI+JvwM1AVeMGDeT3t5W/+wM+vqT1gGuBUyLi9zWJ603AMfl3/1zgCEnn1CCu+cAduWnrGeAaqvvd71s7Ok6qfpGybDepI7unA+nVDdsczQs7KX+Ul1/NCzvCu6mu83Qgcb0ob39A3d6zhm1Op9qO8IG8ZxsAt5E6m1cDpgB71yCuE4BL8/JI4G5g+3bFVdj2MpbvCL8/v28b5OUN2/mz7CO2EcANwLGd+N3vLa6GuvFU2xE+kPdreN5+47x+KXB01e9d01jacZKWBA57AXNI7Z6n5LIzgH3y8pqkO33mAn8Etirse0rebzbwrjrEBXyO1A5+R+H14jrE1nCM06kwaVTwszyM1Dl/F/DlOsRFupPlxzmuu4HPtDmu15M+iS4lXfnMKuz7wRzvXOADVcY1kNjyz/Hpht//HTsdV8MxxlNh0qjgZ/kO0t2DfyIllRFV/zybvTyMiJmZlbaq9mmYmVkHOGmYmVlpThpmZlaak4aZmZXmpGFmZqU5aVjlJI2XFL28du/HceZJuqyFoTaerxjnM5Lul3RpfuiyyvN05XOML5SNl/TBJtv2vJddVcawgvh2afJePCjpm5I2WMljHivpgKpjtfbzMCLWSu8j3WNedHcnAumHy4Bvk/42diSNt/VmSTtGxD8rOsfDpCeNi0/9j8/nvKRh22vztg9XdO7++AQwnfTw5G6khxY3B/5zJY51LPA74CeVRWcd4aRhrXRHRMztdBD9tCCeH8Lid5IeJyWSd1HRP7xIA1GWGiYjIhYBi6o470q4p/Be3CjpxcB/SXppRPylQzFZh7l5ytpO0h6SJkt6WNKTku6SdPyKRumU9FKlyY0WSnoq7/+L/M+sZ5u1JX0pNy0ty19PyaOVrozp+esr8vE3kXSFpMU5hpmSDutPnI3NU5KmAm8D3lJoEpqa617QPCXpWkm3NXlvNsnNSMcVyraU9D1Ji3Icd0jafyXfB0hDtgBsUTjH6yVdpTRB0T+VJhT6oqS1CtvMA14GHFr4/i4r1O8gaZKkR/Mxbpb01gHEaS3kKw1rpeF5IL8eEWm+i61I4wx9A/gXaTC/00nzJyw3e1nBBNI/n8+QhpR+CanZZG3496CB15Em2jqTNLzCG4H/IY27dPxKfA9b5q+PSRoJ3EQat+nkHMNhwARJa0fERWXibOIoYCJpPKGP5rLe5t+YAPxA0piIKDb1HZK/fh9A0ubAH0jzUhxHulo5ELha0n4RManE996oizR677xC2RakIT8uAx4nje12KulnfFDeZn/SYJJ3kn7O5HiQ9Frgt8DtwIdJoxV/DJgi6c0RcetKxGmt1I6xSvwaWi9S+3w0ef2uybYifXg5BXgUGFaomwdcVlh/AvhEH+c9PJ9n54byU0gTIfU5llfe96wcz5qkhHMPadyfTYFj8ja7NOw3hfTPeXjJOLvyccYXyqb28v70vJddeX0t0rwdZzdsdwdpCO+e9e+S/jFv1LDd9aRmw77eh13yOffI78W6wH6kRHZuH/v1/CwPI00atFGhbh4wsck+N+T3eEShbHguu6bTv8t+Lf9y85S10v6kAdd6Xh+CfzelfFvSA6R/5k8DXyCN9PviXo4FqanoM5I+Kek10nITHOwJPAD8P0mr9bxIM66tTkoCK3JyjuefwC15ea9Is8ntTOrzmNqwz0TSVdKYknGutEid8VeRmnp6ZlV8DbAD6Sqkx56kT/d/b3gvrgN2UBqGfEWuI33//yAN2T+NdPX0b0pzZ39J0n3AU3n7CaQEsnVfB89NWG8jDe74XCFGkRLxziVitDZz0rBWuisiZhRes3PfwiTg3aREsSspoZyV91mzj+MdmPf9LGl0zwWSTi30V7yY1Cz0dMPrj7m+cYbEZi7J8ewEjIqI7SPiply3Ic3vYvpLob5MnAM1gXQX0y55/XBS09A1hW1eDBzB8u/FV3J9mffiaNJ7sTtpkp+9SU19RZeSmpO+Thp19fV5P+j7Zwnp/Rqej9kY5zHABhW+Z1YR92lYu72c1IdxeERM7CmUtMLbOCNNaHQ0cLSkbYEjSbfELgK+RRo6+n7g/b0cYl6J+B6OiBm91C0Btm1S/tJCfZk4B+om0mxth0m6idSfcVW88JbgR0h9BV/q5RgLeykvmtPzXki6kdQ3c5KkSyPiIUlrkua0Pj0izu/ZKV/5lPEYqRnrQuCKZhtEPWbvswInDWu3ns7gp3sKJK0OHNqfg0TEbNIMgh8DtsvFvyLNr/5ERNxbQayNbgLeJ+ktEXFzofwQUp/Gcs+g9BJnM0+R+g5WKCJC0kTSp/GfkuaVntCw2a9Iz3fMigqeL8nnPI50B9WJpKS4BulK4emGzcc3OcRTpP6Y4jGXSvotqWntNieIVYOThrXbPaR+h7MkPUv6h3Nc37uApPVJ7dzfA+7N++1LupPp13mz7wEfIM0V/lXS3TojSFc3+wD7RZoWdmVdBnwS+ImkU0gPLh5Kapb5aEQ8WzLOZu4GjpJ0IOmhv8dzwunNBFL/y/+RrjqmNtSfSmqWmybpAtJV1gakxLVVRCz39PmKRMQdkq4GPiTprIhYKOn3wPGSHgYWkyZ52qyX7++tkt5Nas5bHBHzgE+R+kquk/RdUvPfKNLUpcMjoq+76awTOt0T79fge/H8HT+v6KV+R9LTwU+S/vGeAfwXhbuE8nbzyHdPkT7Vfps0G94TpM7Z6cAhDcdek3Rb572kT7dL8nanA6utIO4AvrCCbTYh/cNenI8/EzisUL/COGl+99RLSR3Xj+e6qQ3vZVeTWKbnui/2Euto4DukeaeXkf4hX1+Mt5f9dsnH3VlsdwYAAABxSURBVL1J3atIt92eX/hefpnj/htwAanv4wV3mQGvJDWXPZnrLms45pV5/6fy78Qk0g0IHf999uuFL8/cZ2ZmpfnOBDMzK81Jw8zMSnPSMDOz0pw0zMysNCcNMzMrzUnDzMxKc9IwM7PSnDTMzKy0/w9kiB1pjnFs9QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}